#!/usr/bin/env python3
"""
eDNA Biodiversity Assessment System - Streamlit GUI
A comprehensive web interface for biological sequence analysis
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import tempfile
import os
from pathlib import Path
import time
import json
from io import StringIO
import base64
import streamlit.components.v1 as components

# Project config
from src.utils.config import config as app_config

# Add project modules
import sys
sys.path.append(str(Path(__file__).parent))

# ML Training imports
try:
    from src.models.tokenizer import DNATokenizer
    from src.models.embeddings import DNAContrastiveModel, DNATransformerEmbedder, DNAAutoencoder
    from src.models.trainer import EmbeddingTrainer
except ImportError:
    pass  # Will be handled in the training page if needed

# SRA Integration
try:
    from src.utils.sra_integration import SRAIntegrationUI, create_sra_data_source_selector
except ImportError:
    SRAIntegrationUI = None
    create_sra_data_source_selector = None

try:
    from src.analysis.dataset_analyzer import DatasetAnalyzer
except ImportError as e:
    st.error(f"Failed to import DatasetAnalyzer: {e}")
    st.error("Please ensure the src/analysis/dataset_analyzer.py file exists")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="eDNA Biodiversity Assessment",
    page_icon="ðŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Plotly dark theme and high-contrast palette
import plotly.io as pio
pio.templates.default = "plotly_dark"
px.defaults.template = "plotly_dark"
px.defaults.color_discrete_sequence = (
    px.colors.qualitative.Bold + px.colors.qualitative.Set3 + px.colors.qualitative.Vivid
)

# Custom CSS for "Deep Ocean" UI styling
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600;700&family=Roboto+Mono:wght@400;500&display=swap');

    /* Global Font Settings */
    html, body, [class*="css"] {
        font-family: 'Montserrat', sans-serif;
    }
    
    /* App background - Deep Ocean Gradient */
    [data-testid="stAppViewContainer"] {
        background: linear-gradient(135deg, #050A14 0%, #0A192F 50%, #020C1B 100%);
        color: #E6F1FF !important;
    }
    
    /* Header - Transparent */
    [data-testid="stHeader"] {
        background: rgba(5, 10, 20, 0.8) !important;
        backdrop-filter: blur(10px);
    }
    
    /* Sidebar - Glassmorphism */
    [data-testid="stSidebar"] {
        background-color: rgba(10, 25, 47, 0.95) !important;
        border-right: 1px solid rgba(100, 255, 218, 0.1);
    }

    /* Headings */
    h1, h2, h3 {
        color: #64FFDA !important; /* Neon Teal */
        font-weight: 700 !important;
        letter-spacing: -0.5px;
    }
    
    .main-header {
        font-family: 'Montserrat', sans-serif;
        font-size: 3rem;
        background: linear-gradient(90deg, #64FFDA, #00B4D8);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 800;
        text-shadow: 0 0 20px rgba(100, 255, 218, 0.3);
    }

    /* Buttons - Neon Glow */
    .stButton>button {
        background: rgba(100, 255, 218, 0.1) !important;
        color: #64FFDA !important;
        border: 1px solid #64FFDA !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        transition: all 0.3s ease !important;
        text-transform: uppercase;
        letter-spacing: 1px;
        font-size: 0.9rem !important;
    }
    .stButton>button:hover {
        background: rgba(100, 255, 218, 0.2) !important;
        box-shadow: 0 0 15px rgba(100, 255, 218, 0.4) !important;
        transform: translateY(-2px);
    }
    
    /* Primary Button */
    .stButton>button[kind="primary"] {
        background: linear-gradient(45deg, #64FFDA, #00B4D8) !important;
        color: #020C1B !important;
        border: none !important;
    }
    .stButton>button[kind="primary"]:hover {
        box-shadow: 0 0 20px rgba(100, 255, 218, 0.6) !important;
    }

    /* Inputs & Selects */
    .stSelectbox, .stTextInput, .stNumberInput, .stTextArea {
        color: #E6F1FF !important;
    }
    div[data-baseweb="select"] > div {
        background-color: rgba(255, 255, 255, 0.05) !important;
        border-color: rgba(100, 255, 218, 0.2) !important;
        color: #E6F1FF !important;
    }
    div[data-baseweb="input"] > div {
        background-color: rgba(255, 255, 255, 0.05) !important;
        border-color: rgba(100, 255, 218, 0.2) !important;
        color: #E6F1FF !important;
    }

    /* Cards/Containers */
    div[data-testid="stExpander"] {
        background-color: rgba(17, 34, 64, 0.6) !important;
        border: 1px solid rgba(100, 255, 218, 0.1) !important;
        border-radius: 10px !important;
    }

    /* Info boxes with glassmorphism */
    .success-box {
        background: rgba(27, 94, 32, 0.2);
        backdrop-filter: blur(5px);
        border-left: 4px solid #64FFDA;
        border-radius: 4px;
        padding: 1rem;
        margin: 1rem 0;
        color: #E6F1FF;
    }
    .info-box {
        background: rgba(1, 87, 155, 0.2);
        backdrop-filter: blur(5px);
        border-left: 4px solid #00B4D8;
        border-radius: 4px;
        padding: 1rem;
        margin: 1rem 0;
        color: #E6F1FF;
    }
    
    /* Metrics */
    [data-testid="stMetricValue"] {
        color: #64FFDA !important;
        font-family: 'Roboto Mono', monospace;
    }
    
    /* Tabs */
    .stTabs [data-baseweb="tab-list"] {
        gap: 10px;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: rgba(255,255,255,0.05);
        border-radius: 4px 4px 0 0;
        color: #8892b0;
    }
    .stTabs [aria-selected="true"] {
        background-color: rgba(100, 255, 218, 0.1) !important;
        color: #64FFDA !important;
        border-bottom: 2px solid #64FFDA !important;
    }

    /* Sidebar Navigation Styling */
    section[data-testid="stSidebar"] .stRadio {
        background-color: transparent !important;
    }
    
    section[data-testid="stSidebar"] .stRadio > div[role="radiogroup"] {
        gap: 10px;
    }

    section[data-testid="stSidebar"] .stRadio label {
        background-color: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(100, 255, 218, 0.1);
        border-radius: 8px;
        padding: 12px 20px;
        cursor: pointer;
        transition: all 0.3s ease;
        color: #8892b0 !important;
        font-family: 'Montserrat', sans-serif;
        font-weight: 500;
        display: flex;
        align-items: center;
    }

    section[data-testid="stSidebar"] .stRadio label:hover {
        background-color: rgba(100, 255, 218, 0.1);
        border-color: #64FFDA;
        color: #64FFDA !important;
        transform: translateX(5px);
    }

    /* Selected State */
    section[data-testid="stSidebar"] .stRadio label[data-checked="true"] {
        background: linear-gradient(90deg, rgba(100, 255, 218, 0.2), transparent);
        border-left: 4px solid #64FFDA;
        border-color: rgba(100, 255, 218, 0.2);
        color: #64FFDA !important;
    }
    
    /* Hide the actual radio circle */
    section[data-testid="stSidebar"] .stRadio div[role="radio"] div:first-child {
        display: none;
    }
</style>
""", unsafe_allow_html=True)

def main():
    """Main application function"""
    
    # Initialize session state for navigation (use stable keys to avoid emoji encoding issues)
    if 'current_page_key' not in st.session_state:
        st.session_state.current_page_key = 'home'

    # Header
    st.markdown('<h1 class="main-header">eDNA Biodiversity Assessment System</h1>', unsafe_allow_html=True)
    st.markdown("---")

    # Sidebar navigation
    st.sidebar.markdown("### Navigation")

    PAGES = [
        {"key": "home", "label": "Home"},
        {"key": "analysis", "label": "Dataset Analysis"},
        {"key": "training", "label": "Model Training"},
        {"key": "sra_browser", "label": "SRA Browser"},
        {"key": "batch", "label": "Batch Analysis"},
        {"key": "results", "label": "Results Viewer"},
        {"key": "runs", "label": "Run Browser"},
        {"key": "taxonomy", "label": "Taxonomy Viewer"},
        {"key": "about", "label": "About"},
    ]

    # Find current index
    key_to_index = {p["key"]: i for i, p in enumerate(PAGES)}
    current_index = key_to_index.get(st.session_state.current_page_key, 0)

    selection = st.sidebar.radio(
        "Navigation",
        PAGES,
        index=current_index,
        format_func=lambda p: p["label"],
        key="page_selector",
        label_visibility="collapsed"
    )

    # Update session state when selection changes
    if selection["key"] != st.session_state.current_page_key:
        st.session_state.current_page_key = selection["key"]
        st.rerun()

    page_key = st.session_state.current_page_key

    if page_key == "home":
        show_home_page()
    elif page_key == "analysis":
        show_analysis_page()
    elif page_key == "training":
        show_training_page()
    elif page_key == "sra_browser":
        show_sra_browser_page()
    elif page_key == "batch":
        show_batch_analysis_page()
    elif page_key == "results":
        show_results_viewer()
    elif page_key == "runs":
        show_run_browser()
    elif page_key == "taxonomy":
        show_taxonomy_viewer()
    elif page_key == "about":
        show_about_page()

def show_home_page():
    """Display the home page with navigation and quick links"""
    from pathlib import Path
    
    # Hero Section
    st.markdown("""
    <div style="text-align: center; padding: 2rem 0;">
        <h2 style="color: #64FFDA; font-size: 2rem; margin-bottom: 1rem;">
            Deep Sea Environmental DNA Analysis
        </h2>
        <p style="font-size: 1.2rem; color: #8892b0; max-width: 800px; margin: 0 auto;">
            Advanced biodiversity assessment using next-generation sequencing and machine learning.
            Uncover the secrets of the deep ocean with precision and speed.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Navigation Cards
    st.markdown("### Core Modules")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        with st.container():
            st.markdown("#### Analysis")
            st.info("Process raw sequencing data, perform quality control, and generate taxonomic profiles.")
            if st.button("Launch Analysis", use_container_width=True, key="btn_analysis"):
                st.session_state.current_page_key = "analysis"
                st.rerun()
            
            st.markdown("#### Taxonomy")
            st.info("Explore taxonomic classifications, visualize diversity, and resolve conflicts.")
            if st.button("View Taxonomy", use_container_width=True, key="btn_taxonomy"):
                st.session_state.current_page_key = "taxonomy"
                st.rerun()

    with col2:
        with st.container():
            st.markdown("#### Model Training")
            st.info("Train custom DNA embedding models using Contrastive Learning or Autoencoders.")
            if st.button("Train Models", use_container_width=True, key="btn_training"):
                st.session_state.current_page_key = "training"
                st.rerun()

            st.markdown("#### Results")
            st.info("Interactive visualization of analysis results, abundance plots, and diversity metrics.")
            if st.button("View Results", use_container_width=True, key="btn_results"):
                st.session_state.current_page_key = "results"
                st.rerun()

    with col3:
        with st.container():
            st.markdown("#### Quick Actions")
            st.success("Ready to start? Begin a new analysis workflow immediately.")
            if st.button("Start New Run", type="primary", use_container_width=True, key="btn_start"):
                st.session_state.current_page_key = "analysis"
                st.rerun()
            
            st.markdown("#### History")
            st.info("Browse past analysis runs, logs, and archived reports.")
            if st.button("Browse Runs", use_container_width=True, key="btn_runs"):
                st.session_state.current_page_key = "runs"
                st.rerun()
    
    # Recent runs quick links (from configured storage.runs_dir)
    st.markdown("---")
    st.markdown("### Recent Activity")
    runs_root = Path(app_config.get('storage.runs_dir', 'runs'))
    try:
        if runs_root.exists():
            # Find run folders two levels deep: runs_root/dataset_name/timestamp
            candidates = []
            for dataset_dir in runs_root.iterdir():
                if dataset_dir.is_dir():
                    for run_dir in dataset_dir.iterdir():
                        if run_dir.is_dir():
                            try:
                                mtime = run_dir.stat().st_mtime
                            except Exception:
                                mtime = 0
                            candidates.append((mtime, dataset_dir.name, run_dir))
            candidates.sort(reverse=True)
            top = candidates[:6]
            if top:
                cols = st.columns(3)
                for idx, (_, ds_name, run_path) in enumerate(top):
                    with cols[idx % 3]:
                        label = f"{ds_name} / {run_path.name}"
                        if st.button(f"{label}", key=f"recent_{idx}", use_container_width=True):
                            st.session_state.prefill_results_dir = str(run_path.resolve())
                            st.session_state.current_page_key = "results"
                            st.rerun()
            else:
                st.info(f"No runs found in {runs_root}")
        else:
            st.info(f"Runs directory not found: {runs_root}")
    except Exception as e:
        st.warning(f"Could not list recent runs: {e}")
    
    # Feature overview
    st.markdown("---")
    st.markdown("## Features")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("""
        ### Sequence Analysis
        - Multi-format support (FASTA, FASTQ, Swiss-Prot)
        - Automatic format detection
        - Quality assessment
        - Composition analysis
        """)
    with col2:
        st.markdown("""
        ### Biodiversity Metrics
        - Shannon and Simpson indices
        - Species richness
        - Evenness measures
        """)
    with col3:
        st.markdown("""
        ### Advanced Features
        - Interactive visualizations
        - Batch analysis (planned)
        - Export capabilities
        """)

def show_analysis_page():
    """Display the main analysis page"""
    
    try:
        st.title("Dataset Analysis")
        
        # Resolve storage directories from config
        datasets_dir = Path(app_config.get('storage.datasets_dir', 'data/datasets'))
        runs_root = Path(app_config.get('storage.runs_dir', 'runs'))
        datasets_dir.mkdir(parents=True, exist_ok=True)
        runs_root.mkdir(parents=True, exist_ok=True)
        
        # File upload section
        st.markdown("## 1. Upload Your Dataset")
        
        # Add data source selection with SRA support
        data_source_type = st.radio(
            "Data Source",
            ["Upload File", "Download from SRA"],
            horizontal=True,
            help="Choose between uploading a local file or downloading from NCBI SRA"
        )
    
        # Add info about potential upload issues
        with st.expander("Troubleshooting Upload Issues"):
            st.markdown("""
            **If you encounter upload errors:**
            - **AxiosError or Request Failed**: Usually indicates a network timeout for large files
            - **Solutions**:
              - Try with a smaller file first (< 100MB)
              - Check your internet connection stability
              - For very large files, consider using the command-line interface
              - Refresh the page and try again
            
            **File Size Limits:**
            - Maximum supported: 10GB
            - Recommended for web upload: < 1GB
            - For larger files, use: `python scripts/analyze_dataset.py your_file.fasta output_report.txt`
            """)
        
        uploaded_file = None
        sra_file_path = None
        
        if data_source_type == "Upload File":
            uploaded_file = st.file_uploader(
                "Choose a biological sequence file",
                type=['fasta', 'fa', 'fas', 'fastq', 'fq', 'swiss', 'gb', 'gbk', 'embl', 'em', 'gz'],
                help="Supported formats: FASTA, FASTQ, Swiss-Prot, GenBank, EMBL (including gzipped files). Maximum file size: 10GB"
            )
        else:
            # SRA download interface
            if SRAIntegrationUI:
                sra_ui = SRAIntegrationUI()
                sra_ui.show_sra_toolkit_status()
                
                if sra_ui.sra_toolkit_available:
                    st.markdown("#### Quick SRA Download")
                    accession_input = st.text_input(
                        "Enter SRA Accession",
                        placeholder="e.g., SRR12345678",
                        help="Enter a specific SRA accession number to download"
                    )
                    
                    if accession_input and st.button("Download SRA Dataset", type="primary"):
                        output_dir = Path("data/sra") / accession_input
                        status_text = st.empty()
                        progress_bar = st.progress(0)
                        
                        def update_progress(msg):
                            status_text.text(msg)
                        
                        progress_bar.progress(10)
                        success, file_path = sra_ui.download_sra_dataset(
                            accession_input,
                            output_dir,
                            progress_callback=update_progress
                        )
                        
                        if success:
                            progress_bar.progress(100)
                            status_text.text("Download complete!")
                            st.success(f"Successfully downloaded {accession_input}")
                            sra_file_path = file_path
                            # Store in session state for analysis
                            st.session_state.sra_downloaded_file = file_path
                        else:
                            status_text.text("Download failed")
                            st.error(f"Failed to download {accession_input}")
                    
                    # Use previously downloaded file
                    if 'sra_downloaded_file' in st.session_state:
                        sra_file_path = st.session_state.sra_downloaded_file
                        st.info(f"Using downloaded file: {sra_file_path.name}")
            else:
                st.warning("SRA integration not available. Please check installation.")
        
        # File size information
        file_valid = True
        if uploaded_file is not None:
            try:
                # Check file size
                file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
                file_size_gb = file_size_mb / 1024
                
                if file_size_gb >= 1:
                    st.info(f"File size: {file_size_gb:.2f} GB ({file_size_mb:.0f} MB)")
                else:
                    st.info(f"File size: {file_size_mb:.2f} MB")
                
                if file_size_mb > 10240:  # 10GB limit
                    st.error("File size exceeds 10GB limit. Please use a smaller file or contact support for processing very large datasets.")
                    file_valid = False
                elif file_size_mb > 1024:  # Warn for files over 1GB
                    st.warning(f"Large file detected ({file_size_mb:.0f} MB). Upload may take longer than usual. Please be patient.")
                    
            except Exception as e:
                st.error(f"Error reading file: {str(e)}")
                st.error("This might be due to a network timeout for large files. Please try with a smaller file or check your connection.")
                file_valid = False
        
        # Analysis configuration
        st.markdown("## 2. Analysis Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            dataset_name = st.text_input(
                "Dataset Name",
                value="My Dataset",
                help="Custom name for your analysis"
            )
            
            max_sequences = st.number_input(
                "Maximum Sequences to Analyze",
                min_value=100,
                max_value=1000000,
                value=None,
                help="Leave empty to analyze all sequences"
            )
        
        with col2:
            format_override = st.selectbox(
                "Force Format (Optional)",
                ["Auto-detect", "FASTA", "FASTQ", "Swiss-Prot", "GenBank", "EMBL"],
                help="Override automatic format detection"
            )
            
            analysis_level = st.selectbox(
                "Analysis Depth",
                ["Standard", "Detailed", "Quick"],
                index=0,
                help="Choose analysis comprehensiveness"
            )
        
        # Advanced options
        with st.expander("Advanced Options"):
            col1, col2 = st.columns(2)
            
            with col1:
                enable_quality = st.checkbox("Quality Analysis", value=True)
                enable_diversity = st.checkbox("Biodiversity Metrics", value=True)
                fast_mode = st.checkbox("Fast Mode (Recommended for >10K sequences)", value=True, 
                                      help="Uses intelligent sampling to dramatically reduce analysis time for large datasets")
            
            with col2:
                enable_visualization = st.checkbox("Generate Plots", value=True)
                parallel_processing = st.checkbox("Parallel Processing", value=True)
        
        # Show fast mode info
        if fast_mode:
            st.info("Fast Mode enabled: Analysis will use intelligent sampling for datasets >10K sequences, reducing time by 60-80%")
        
        # Analysis execution
        st.markdown("## 3. Run Analysis")
        
        # Determine which file to use
        analysis_file = uploaded_file if uploaded_file is not None else sra_file_path
        
        if analysis_file is not None and file_valid:
            if st.button("Start Analysis", type="primary", use_container_width=True):
                # Handle both uploaded files and SRA file paths
                if isinstance(analysis_file, Path):
                    # SRA file - already on disk
                    run_analysis_from_path(analysis_file, dataset_name, max_sequences, format_override, 
                                analysis_level, enable_quality, enable_diversity, enable_visualization, fast_mode)
                else:
                    # Uploaded file - use existing run_analysis function
                    run_analysis(analysis_file, dataset_name, max_sequences, format_override, 
                                analysis_level, enable_quality, enable_diversity, enable_visualization, fast_mode)
        elif analysis_file is not None and not file_valid:
            st.warning("Cannot proceed with analysis due to file size restriction.")
        else:
            st.info("Please upload a file or download from SRA to start analysis")
        
    except Exception as e:
        st.error(f"Error in analysis page: {str(e)}")
        import traceback
        with st.expander("Show Error Details"):
            st.code(traceback.format_exc())

def run_analysis(uploaded_file, dataset_name, max_sequences, format_override, 
                analysis_level, enable_quality, enable_diversity, enable_visualization, fast_mode=True):
    """Execute the analysis with progress tracking and persist inputs/outputs to configured storage"""
    
    # Resolve storage roots
    datasets_dir = Path(app_config.get('storage.datasets_dir', 'data/datasets'))
    runs_root = Path(app_config.get('storage.runs_dir', 'runs'))
    datasets_dir.mkdir(parents=True, exist_ok=True)
    runs_root.mkdir(parents=True, exist_ok=True)

    # Build run directory
    safe_name = ''.join(c if c.isalnum() or c in ('-','_') else '_' for c in (dataset_name or 'dataset'))
    ts = time.strftime('%Y%m%d_%H%M%S')
    run_dir = runs_root / safe_name / ts
    run_dir.mkdir(parents=True, exist_ok=True)

    # Persist uploaded file to datasets_dir
    try:
        if '.' in uploaded_file.name:
            file_ext = uploaded_file.name.split('.')[-1]
        else:
            file_ext = 'fasta'
        dataset_path = datasets_dir / f"{safe_name}_{ts}.{file_ext}"
        with open(dataset_path, 'wb') as f:
            f.write(uploaded_file.getvalue())
    except Exception as e:
        st.error(f"âœ‚ Failed to store uploaded file: {e}")
        return

    # Prepare output report path inside run directory
    output_path = str(run_dir / 'analysis_report.txt')

    try:
        # Initialize progress
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Initialize analyzer with fast mode
        status_text.text("Initializing analyzer...")
        analyzer = DatasetAnalyzer(fast_mode=fast_mode)
        if fast_mode:
            st.info("Fast mode enabled - using optimized processing for large datasets")
        progress_bar.progress(10)
        
        # Prepare parameters
        format_type = None if format_override == "Auto-detect" else format_override.lower()
        max_seq = None if max_sequences == 0 else max_sequences
        
        # Run analysis with progress monitoring
        status_text.text("Running analysis...")
        progress_bar.progress(30)
        
        # Add warning for large files
        if uploaded_file and len(uploaded_file.getvalue()) > 100 * 1024 * 1024:  # > 100MB
            st.warning("Large file detected. Analysis may take several minutes. Please be patient...")
            if len(uploaded_file.getvalue()) > 200 * 1024 * 1024:  # > 200MB
                st.warning("For files > 200MB, consider using a smaller subset or the command-line tool for better performance.")
        
        start_time = time.time()
        
        # Execute analysis
        results = analyzer.analyze_dataset(
            input_path=str(dataset_path),
            output_path=output_path,
            dataset_name=dataset_name,
            format_type=format_type,
            max_sequences=max_seq
        )
        
        if not results or 'basic_stats' not in results:
            raise ValueError("Analysis did not complete successfully or returned incomplete results")
        
        progress_bar.progress(80)
        status_text.text("Generating visualizations...")
        
        # Display results
        display_results(results, output_path, enable_visualization)
        
        progress_bar.progress(100)
        status_text.text("Analysis complete!")
        
        elapsed_time = time.time() - start_time
        st.success(f"Analysis completed in {elapsed_time:.2f} seconds!")
        st.info(f"Inputs saved to: {dataset_path}")
        st.info(f"Run outputs: {run_dir}")
        
    except Exception as e:
        st.error(f"Analysis failed: {str(e)}")
        import traceback
        st.error(f"Error details: {traceback.format_exc()}")

def run_analysis_from_path(file_path, dataset_name, max_sequences, format_override, 
                          analysis_level, enable_quality, enable_diversity, enable_visualization, fast_mode=True):
    """Execute analysis from a file path (e.g., SRA downloaded file)"""
    
    # Resolve storage roots
    runs_root = Path(app_config.get('storage.runs_dir', 'runs'))
    runs_root.mkdir(parents=True, exist_ok=True)

    # Build run directory
    safe_name = ''.join(c if c.isalnum() or c in ('-','_') else '_' for c in (dataset_name or 'dataset'))
    ts = time.strftime('%Y%m%d_%H%M%S')
    run_dir = runs_root / safe_name / ts
    run_dir.mkdir(parents=True, exist_ok=True)

    # Prepare output report path inside run directory
    output_path = str(run_dir / 'analysis_report.txt')

    try:
        # Initialize progress
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Initialize analyzer with fast mode
        status_text.text("Initializing analyzer...")
        analyzer = DatasetAnalyzer(fast_mode=fast_mode)
        if fast_mode:
            st.info("Fast mode enabled - using optimized processing for large datasets")
        progress_bar.progress(10)
        
        # Prepare parameters
        format_type = None if format_override == "Auto-detect" else format_override.lower()
        max_seq = None if max_sequences == 0 else max_sequences
        
        # Run analysis with progress monitoring
        status_text.text("Running analysis...")
        progress_bar.progress(30)
        
        start_time = time.time()
        
        # Execute analysis
        results = analyzer.analyze_dataset(
            input_path=str(file_path),
            output_path=output_path,
            dataset_name=dataset_name,
            format_type=format_type,
            max_sequences=max_seq
        )
        
        if not results or 'basic_stats' not in results:
            raise ValueError("Analysis did not complete successfully or returned incomplete results")
        
        progress_bar.progress(80)
        status_text.text("Generating visualizations...")
        
        # Display results
        display_results(results, output_path, enable_visualization)
        
        progress_bar.progress(100)
        status_text.text("Analysis complete!")
        
        elapsed_time = time.time() - start_time
        st.success(f"Analysis completed in {elapsed_time:.2f} seconds!")
        st.info(f"Input file: {file_path}")
        st.info(f"Run outputs: {run_dir}")
        
    except Exception as e:
        st.error(f"Analysis failed: {str(e)}")
        import traceback
        st.error(f"Error details: {traceback.format_exc()}")


def display_results(results, output_path, enable_visualization):
    """Display analysis results with visualizations"""
    
    st.markdown("---")
    st.markdown("## Analysis Results")
    
    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)
    
    stats = results.get('basic_stats', {})
    comp = results.get('composition', {})
    proc = results.get('processing_info', {})
    
    with col1:
        st.metric(
            "Total Sequences",
            f"{stats.get('total_sequences', 0):,}"
        )
    
    with col2:
        st.metric(
            "Mean Length",
            f"{stats.get('mean_length', 0):.1f}",
            f"Â±{stats.get('std_length', 0):.1f}"
        )
    
    with col3:
        st.metric(
            "Sequence Type",
            comp.get('sequence_type', 'Unknown').upper()
        )
    
    with col4:
        st.metric(
            "Processing Time",
            f"{proc.get('total_time', 0):.2f}s"
        )
    
    # Detailed results tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "Statistics", "Composition", "Annotations", 
        "Biodiversity", "Report"
    ])
    
    with tab1:
        show_statistics_tab(results, enable_visualization)
    
    with tab2:
        show_composition_tab(results, enable_visualization)
    
    with tab3:
        show_annotations_tab(results)
    
    with tab4:
        show_biodiversity_tab(results)
    
    with tab5:
        show_report_tab(output_path)

def show_statistics_tab(results, enable_visualization):
    """Display statistics tab"""
    
    stats = results.get('basic_stats', {})
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Length Statistics")
        
        stats_df = pd.DataFrame({
            'Metric': ['Total Sequences', 'Minimum Length', 'Maximum Length', 
                      'Mean Length', 'Median Length', 'Std Deviation'],
            'Value': [
                f"{stats.get('total_sequences', 0):,}",
                f"{stats.get('min_length', 0):,}",
                f"{stats.get('max_length', 0):,}",
                f"{stats.get('mean_length', 0):.2f}",
                f"{stats.get('median_length', 0):.2f}",
                f"{stats.get('std_length', 0):.2f}"
            ]
        })
        
        st.dataframe(stats_df, hide_index=True, use_container_width=True)
    
    with col2:
        if enable_visualization and stats:
            st.markdown("### Length Distribution")
            
            # Create synthetic data for demonstration
            # In real implementation, you'd pass the actual sequence lengths
            lengths = np.random.normal(
                stats.get('mean_length', 300), 
                stats.get('std_length', 100), 
                min(1000, stats.get('total_sequences', 100))
            )
            lengths = lengths[lengths > 0]  # Remove negative values
            
            fig = px.histogram(
                x=lengths,
                nbins=50,
                title="Sequence Length Distribution",
                labels={'x': 'Sequence Length', 'y': 'Frequency'}
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

def show_composition_tab(results, enable_visualization):
    """Display composition analysis tab"""
    
    comp = results.get('composition', {})
    
    if comp.get('composition'):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown(f"### {comp.get('sequence_type', 'Unknown').upper()} Composition")
            
            # Create composition dataframe
            comp_data = []
            for char, freq in list(comp['composition'].items())[:15]:  # Top 15
                comp_data.append({
                    'Character': char,
                    'Frequency': freq,
                    'Percentage': f"{freq*100:.3f}%"
                })
            
            comp_df = pd.DataFrame(comp_data)
            st.dataframe(comp_df, hide_index=True, use_container_width=True)
        
        with col2:
            if enable_visualization:
                st.markdown("### Composition Chart")
                
                # Create pie chart for top characters
                top_chars = list(comp['composition'].items())[:10]
                chars, freqs = zip(*top_chars)
                
                fig = px.pie(
                    values=freqs,
                    names=chars,
                    title=f"{comp.get('sequence_type', 'Unknown').upper()} Character Distribution",
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                st.plotly_chart(fig, use_container_width=True)

def show_annotations_tab(results):
    """Display annotations tab"""
    
    ann = results.get('annotations', {})
    
    st.markdown("### Annotation Summary")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "With Organisms",
            f"{ann.get('sequences_with_organisms', 0):,}"
        )
    
    with col2:
        st.metric(
            "With Descriptions",
            f"{ann.get('sequences_with_descriptions', 0):,}"
        )
    
    with col3:
        st.metric(
            "With Features",
            f"{ann.get('sequences_with_features', 0):,}"
        )
    
    # Organism distribution
    if ann.get('organism_distribution'):
        st.markdown("### Top Organisms")
        
        org_data = []
        for org, count in ann['organism_distribution'].items():
            org_data.append({'Organism': org, 'Count': count})
        
        org_df = pd.DataFrame(org_data)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.dataframe(org_df, hide_index=True, use_container_width=True)
        
        with col2:
            if len(org_data) > 1:  # Only show chart if multiple organisms
                fig = px.bar(
                    org_df,
                    x='Count',
                    y='Organism',
                    orientation='h',
                    title="Organism Distribution"
                )
                st.plotly_chart(fig, use_container_width=True)

def show_biodiversity_tab(results):
    """Display biodiversity metrics tab"""
    
    div = results.get('diversity', {})
    
    if div:
        st.markdown("### Biodiversity Metrics")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric(
                "Species Richness",
                f"{div.get('species_richness', 0):,}"
            )
            
            st.metric(
                "Shannon Diversity",
                f"{div.get('shannon_diversity', 0):.4f}"
            )
        
        with col2:
            st.metric(
                "Simpson Diversity",
                f"{div.get('simpson_diversity', 0):.4f}"
            )
            
            st.metric(
                "Evenness",
                f"{div.get('evenness', 0):.4f}"
            )
        
        # Create diversity comparison chart
        diversity_data = {
            'Metric': ['Shannon Diversity', 'Simpson Diversity', 'Evenness'],
            'Value': [
                div.get('shannon_diversity', 0),
                div.get('simpson_diversity', 0),
                div.get('evenness', 0)
            ]
        }
        
        fig = px.bar(
            diversity_data,
            x='Metric',
            y='Value',
            title="Biodiversity Metrics Comparison"
        )
        st.plotly_chart(fig, use_container_width=True)

def show_report_tab(output_path):
    """Display report tab with download option"""
    
    st.markdown("### Analysis Report")
    
    try:
        # Read the report file
        with open(output_path, 'r', encoding='utf-8') as f:
            report_content = f.read()
        
        # Display report content
        st.text_area(
            "Generated Report",
            value=report_content,
            height=400,
            help="Complete analysis report"
        )
        
        # Download button
        st.download_button(
            label="Download Report",
            data=report_content,
            file_name=f"analysis_report_{int(time.time())}.txt",
            mime="text/plain",
            type="primary",
            use_container_width=True
        )
        
    except Exception as e:
        st.error(f"Could not load report: {str(e)}")

def show_sra_browser_page():
    """Display SRA dataset browser and batch download interface"""
    
    st.title("NCBI SRA Dataset Browser")
    st.markdown("""
    Search and download datasets from NCBI Sequence Read Archive (SRA).
    Find eDNA and metabarcoding datasets for analysis or model training.
    """)
    
    if not SRAIntegrationUI:
        st.error("SRA integration module not available. Please check installation.")
        return
    
    sra_ui = SRAIntegrationUI()
    
    # Check toolkit status
    sra_ui.show_sra_toolkit_status()
    
    if not sra_ui.sra_toolkit_available:
        st.info("Install SRA Toolkit to enable dataset downloads.")
        return
    
    # Create tabs for different functionalities
    tab1, tab2, tab3 = st.tabs(["Search & Browse", "Batch Download", "Downloaded Datasets"])
    
    with tab1:
        st.markdown("### Search NCBI SRA")
        
        # Search interface
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            search_terms = st.text_input(
                "Search Keywords",
                placeholder="eDNA, 18S rRNA, marine metabarcoding",
                help="Enter comma-separated keywords to search"
            )
        
        with col2:
            max_results = st.number_input("Max Results", min_value=10, max_value=100, value=30)
        
        with col3:
            st.markdown("&nbsp;")  # Spacing
            search_button = st.button("Search", type="primary", use_container_width=True)
        
        if search_button:
            keywords = [term.strip() for term in search_terms.split(',') if term.strip()]
            
            with st.spinner("Searching NCBI SRA database..."):
                results = sra_ui.search_sra_datasets(keywords, max_results)
                st.session_state.sra_search_results = results
        
        # Display results
        if 'sra_search_results' in st.session_state and st.session_state.sra_search_results:
            results = st.session_state.sra_search_results
            st.success(f"Found {len(results)} datasets")
            
            # Add filters
            st.markdown("#### Filters")
            col1, col2 = st.columns(2)
            
            with col1:
                organism_filter = st.multiselect(
                    "Filter by Organism",
                    options=list(set(r.get('organism', 'Unknown') for r in results)),
                    default=[]
                )
            
            with col2:
                platform_filter = st.multiselect(
                    "Filter by Platform",
                    options=list(set(r.get('platform', 'Unknown') for r in results)),
                    default=[]
                )
            
            # Apply filters
            filtered_results = results
            if organism_filter:
                filtered_results = [r for r in filtered_results if r.get('organism') in organism_filter]
            if platform_filter:
                filtered_results = [r for r in filtered_results if r.get('platform') in platform_filter]
            
            st.markdown(f"#### Results ({len(filtered_results)} datasets)")
            
            # Display in expandable cards
            for idx, study in enumerate(filtered_results):
                with st.expander(
                    f"**{study.get('accession', 'Unknown')}** - {study.get('title', 'No title')[:100]}..."
                ):
                    col1, col2, col3 = st.columns([2, 1, 1])
                    
                    with col1:
                        st.markdown(f"**Accession:** `{study.get('accession', 'N/A')}`")
                        st.markdown(f"**Organism:** {study.get('organism', 'N/A')}")
                        st.markdown(f"**Platform:** {study.get('platform', 'N/A')}")
                        st.markdown(f"**Title:** {study.get('title', 'N/A')}")
                    
                    with col2:
                        spots = study.get('spots', '0')
                        bases = study.get('bases', '0')
                        st.metric("Spots", f"{int(spots):,}" if spots.isdigit() else spots)
                        st.metric("Bases", f"{int(bases):,}" if bases.isdigit() else bases)
                    
                    with col3:
                        # Download button
                        if st.button(f"Download", key=f"dl_{idx}"):
                            accession = study.get('accession')
                            output_dir = Path("data/sra") / accession
                            
                            status_text = st.empty()
                            progress_bar = st.progress(0)
                            
                            def update_progress(msg):
                                status_text.text(msg)
                            
                            progress_bar.progress(10)
                            success, file_path = sra_ui.download_sra_dataset(
                                accession,
                                output_dir,
                                progress_callback=update_progress
                            )
                            
                            if success:
                                progress_bar.progress(100)
                                status_text.text("Download complete!")
                                st.success(f"Downloaded to {file_path}")
                                
                                # Add to batch download list
                                if 'downloaded_sra' not in st.session_state:
                                    st.session_state.downloaded_sra = []
                                st.session_state.downloaded_sra.append({
                                    'accession': accession,
                                    'path': str(file_path),
                                    'metadata': study
                                })
                            else:
                                status_text.text("Download failed")
                                st.error("Download failed")
                        
                        # Add to batch queue
                        if st.button(f"Add to Queue", key=f"queue_{idx}"):
                            if 'sra_batch_queue' not in st.session_state:
                                st.session_state.sra_batch_queue = []
                            
                            if study not in st.session_state.sra_batch_queue:
                                st.session_state.sra_batch_queue.append(study)
                                st.success(f"Added {study.get('accession')} to queue")
                            else:
                                st.warning("Already in queue")
    
    with tab2:
        st.markdown("### Batch Download Queue")
        
        if 'sra_batch_queue' not in st.session_state or not st.session_state.sra_batch_queue:
            st.info("No datasets in queue. Add datasets from the Search tab.")
        else:
            queue = st.session_state.sra_batch_queue
            st.success(f"{len(queue)} datasets in queue")
            
            # Display queue
            for idx, study in enumerate(queue):
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.markdown(f"**{idx+1}.** {study.get('accession')} - {study.get('title', 'No title')[:80]}...")
                with col2:
                    if st.button(f"Remove", key=f"remove_{idx}"):
                        st.session_state.sra_batch_queue.pop(idx)
                        st.rerun()
            
            st.markdown("---")
            
            # Batch download controls
            col1, col2 = st.columns([1, 1])
            
            with col1:
                if st.button("Download All", type="primary", use_container_width=True):
                    st.markdown("### Download Progress")
                    
                    for idx, study in enumerate(queue):
                        accession = study.get('accession')
                        st.markdown(f"**{idx+1}/{len(queue)}:** Downloading {accession}...")
                        
                        output_dir = Path("data/sra") / accession
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        def update_progress(msg):
                            status_text.text(msg)
                        
                        progress_bar.progress(10)
                        success, file_path = sra_ui.download_sra_dataset(
                            accession,
                            output_dir,
                            progress_callback=update_progress
                        )
                        
                        if success:
                            progress_bar.progress(100)
                            status_text.text("Complete")
                            st.success(f"Downloaded {accession}")
                            
                            # Add to downloaded list
                            if 'downloaded_sra' not in st.session_state:
                                st.session_state.downloaded_sra = []
                            st.session_state.downloaded_sra.append({
                                'accession': accession,
                                'path': str(file_path),
                                'metadata': study
                            })
                        else:
                            status_text.text("Failed")
                            st.error(f"Failed to download {accession}")
                    
                    st.success("Batch download complete!")
                    st.session_state.sra_batch_queue = []
            
            with col2:
                if st.button("Clear Queue", use_container_width=True):
                    st.session_state.sra_batch_queue = []
                    st.rerun()
    
    with tab3:
        st.markdown("### Downloaded Datasets")
        
        if 'downloaded_sra' not in st.session_state or not st.session_state.downloaded_sra:
            st.info("No datasets downloaded yet.")
        else:
            downloads = st.session_state.downloaded_sra
            st.success(f"{len(downloads)} datasets downloaded")
            
            for idx, item in enumerate(downloads):
                with st.expander(f"**{item['accession']}** - {item.get('metadata', {}).get('title', 'No title')[:80]}..."):
                    st.markdown(f"**Path:** `{item['path']}`")
                    st.markdown(f"**Organism:** {item.get('metadata', {}).get('organism', 'N/A')}")
                    st.markdown(f"**Platform:** {item.get('metadata', {}).get('platform', 'N/A')}")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if st.button(f"Analyze Dataset", key=f"analyze_{idx}"):
                            st.info(f"Navigate to 'Dataset Analysis' page to analyze {item['path']}")
                    
                    with col2:
                        if st.button(f"Use for Training", key=f"train_{idx}"):
                            st.info(f"Navigate to 'Model Training' page to use this dataset")

def show_batch_analysis_page():
    """Display batch analysis page"""
    
    st.title("Batch Analysis")
    st.info("Batch analysis feature coming soon! This will allow you to analyze multiple datasets simultaneously.")
    
    # Placeholder for batch analysis interface
    st.markdown("""
    ### Planned Features:
    - **Multiple file upload**
    - **Parallel processing**
    - **Comparative analysis**
    - **Batch report generation**
    """)

def show_results_viewer():
    """Display results viewer page"""
    st.title("Results Viewer")

    # Choose results directory (support prefill from homepage)
    default_dir = str(Path(app_config.get('storage.runs_dir', 'runs')).resolve())
    prefill = st.session_state.get('prefill_results_dir')
    if prefill and Path(prefill).exists():
        default_dir = prefill
    
    results_dir = st.text_input(
        "Results directory",
        value=default_dir,
        help="Folder containing pipeline_results.json and subfolders: clustering, taxonomy, novelty, visualizations",
        key="results_dir_input"
    )
    # Persist last used results dir for quick navigation
    if results_dir:
        st.session_state.prefill_results_dir = results_dir

    if not results_dir:
        return
    base = Path(results_dir)
    if not base.exists():
        st.warning(f"Directory not found: {base}")
        return

    # Helper to check files
    def p(*parts):
        return base.joinpath(*parts)

    st.markdown("---")
    # 1) Pipeline summary
    st.subheader("Pipeline Summary")
    summary_cols = st.columns(5)
    pr_path = p('pipeline_results.json')
    ui_report_path = p('analysis_report.txt')
    try:
        if pr_path.exists():
            with open(pr_path, 'r', encoding='utf-8') as f:
                pr = json.load(f)
            s = pr.get('summary', {})
            summary_cols[0].metric("Total sequences", f"{s.get('total_sequences_processed', 0):,}")
            summary_cols[1].metric("Clusters", f"{s.get('total_clusters', 0):,}")
            summary_cols[2].metric("Taxa identified", f"{s.get('total_taxa_identified', 0):,}")
            summary_cols[3].metric("Novel candidates", f"{s.get('novel_taxa_candidates', 0):,}")
            summary_cols[4].metric("Novel %", f"{s.get('novelty_percentage', 0)}%")
        else:
            # Fallback to UI report-only runs
            if ui_report_path.exists():
                summary_cols[0].metric("Report only", "Yes")
                st.info("Detected UI analysis report (no pipeline_results.json). Showing the text report below.")
            else:
                st.info("pipeline_results.json not found; showing sections based on available files")
    except Exception as e:
        st.warning(f"Could not read pipeline_results.json: {e}")

    # UI text report display
    if ui_report_path.exists():
        st.markdown("### Analysis Report (UI)")
        try:
            st.text_area("analysis_report.txt", ui_report_path.read_text(encoding='utf-8', errors='ignore'), height=240)
            st.download_button("Download analysis_report.txt", data=ui_report_path.read_bytes(), file_name="analysis_report.txt")
        except Exception:
            st.write(f"Open report: {ui_report_path}")

    st.markdown("---")
    # 2) Clustering
    st.subheader("Clustering")
    clus_img = p('clustering', 'cluster_visualization.png')
    clus_stats = p('clustering', 'cluster_stats.txt')
    clus_csv = p('clustering', 'cluster_assignments.csv')

    cols = st.columns([2,1])
    with cols[0]:
        if clus_img.exists():
            st.image(str(clus_img), caption="Cluster visualization")
        else:
            st.info("No cluster_visualization.png found")
    with cols[1]:
        if clus_stats.exists():
            try:
                st.text_area("Cluster stats", clus_stats.read_text(encoding='utf-8'), height=200)
            except Exception:
                st.text("cluster_stats.txt present (could not display)")
        if clus_csv.exists():
            try:
                dfc = pd.read_csv(clus_csv)
                st.write("Cluster assignments (first 10 rows):")
                st.dataframe(dfc.head(10), hide_index=True, use_container_width=True)
                st.download_button("Download cluster_assignments.csv", data=clus_csv.read_bytes(), file_name="cluster_assignments.csv")
            except Exception as e:
                st.warning(f"Could not read cluster_assignments.csv: {e}")

    st.markdown("---")
    # 3) Taxonomy
    st.subheader("Taxonomy")
    tax_csv = p('taxonomy', 'taxonomy_predictions.csv')
    if tax_csv.exists():
        try:
            dft = pd.read_csv(tax_csv)
            # Summary
            tcols = st.columns(4)
            tcols[0].metric("Rows", f"{len(dft):,}")
            if 'assigned_label' in dft.columns:
                tcols[1].metric("Unique taxa", f"{dft['assigned_label'].dropna().nunique():,}")
            if 'tiebreak_winner' in dft.columns:
                try:
                    tb_counts = dft['tiebreak_winner'].value_counts()
                    tcols[2].metric("BLAST wins", f"{int(tb_counts.get('blast',0)):,}")
                    tcols[3].metric("KNN wins", f"{int(tb_counts.get('knn',0)):,}")
                except Exception:
                    pass
            st.write("Top taxa (bar chart)")
            if 'assigned_label' in dft.columns:
                top_counts = dft['assigned_label'].fillna('Unknown').value_counts().head(15)
                chart_df = pd.DataFrame({"Taxon": top_counts.index, "Count": top_counts.values})
                fig = px.bar(chart_df, x='Taxon', y='Count', title="Top 15 Assigned Taxa")
                st.plotly_chart(fig, use_container_width=True)
            st.dataframe(dft.head(50), use_container_width=True, hide_index=True)
            st.download_button("Download taxonomy_predictions.csv", data=tax_csv.read_bytes(), file_name="taxonomy_predictions.csv")
            # Conflicts report
            tb_report = p('taxonomy', 'taxonomy_tiebreak_report.csv')
            if tb_report.exists():
                st.download_button("Download taxonomy_tiebreak_report.csv", data=tb_report.read_bytes(), file_name="taxonomy_tiebreak_report.csv")
        except Exception as e:
            st.warning(f"Could not parse taxonomy_predictions.csv: {e}")
    else:
        st.info("No taxonomy_predictions.csv found")

    st.markdown("---")
    # 4) Novelty
    st.subheader("Novelty")
    nov_json = p('novelty', 'novelty_analysis.json')
    nov_img = p('novelty', 'novelty_visualization.png')
    if nov_json.exists():
        try:
            nov = json.loads(nov_json.read_text(encoding='utf-8'))
            ncols = st.columns(4)
            ncols[0].metric("Total sequences", f"{nov.get('total_sequences',0):,}")
            ncols[1].metric("Novel candidates", f"{nov.get('novel_candidates',0):,}")
            ncols[2].metric("Novel %", f"{nov.get('novel_percentage',0):.2f}%")
            if nov_img.exists():
                st.image(str(nov_img), caption="Novelty visualization")
        except Exception as e:
            st.warning(f"Could not read novelty_analysis.json: {e}")
    else:
        st.info("No novelty analysis found")

    st.markdown("---")
    # 5) Dashboard
    st.subheader("Dashboard")
    dash_html = p('visualizations', 'analysis_dashboard.html')
    if dash_html.exists():
        try:
            components.html(dash_html.read_text(encoding='utf-8'), height=600, scrolling=True)
        except Exception:
            st.write(f"Open dashboard: {dash_html}")
    else:
        st.info("No analysis_dashboard.html found")
    
    st.title("Results Viewer")
    st.info("Results viewer coming soon! This will allow you to browse and compare previous analyses.")
    
    # Placeholder for results viewer
    st.markdown("""
    ### Planned Features:
    - **Analysis history**
    - **Result comparison**
    - **Trend analysis**
    - **Export utilities**
    """)

def show_run_browser():
    """Browse and open runs stored under the configured runs directory"""
    import time as _time
    st.title("Run Browser")

    # Base directory input
    default_root = str(Path(app_config.get('storage.runs_dir', 'runs')).resolve())
    root_dir = st.text_input(
        "Runs root directory",
        value=default_root,
        help="Root folder containing runs organized as <dataset_name>/<timestamp>"
    )

    if not root_dir:
        return
    base = Path(root_dir)
    if not base.exists():
        st.warning(f"Directory not found: {base}")
        return

    # Discover dataset folders
    datasets = sorted([d.name for d in base.iterdir() if d.is_dir()])
    cols_top = st.columns([2,2,2,1])
    with cols_top[0]:
        ds_filter = st.selectbox("Dataset", ["All"] + datasets)
    with cols_top[1]:
        search = st.text_input("Search (dataset/run)", "")
    with cols_top[2]:
        try:
            show_n = st.number_input("Show top N", min_value=5, max_value=500, value=20, step=5)
        except Exception:
            show_n = 20
    with cols_top[3]:
        refresh = st.button("Refresh")

    # Collect runs
    rows = []
    for ds in datasets:
        if ds_filter != "All" and ds != ds_filter:
            continue
        ds_path = base / ds
        try:
            for run_dir in ds_path.iterdir():
                if not run_dir.is_dir():
                    continue
                try:
                    mtime = run_dir.stat().st_mtime
                    mtime_str = _time.strftime('%Y-%m-%d %H:%M:%S', _time.localtime(mtime))
                except Exception:
                    mtime, mtime_str = 0, ""
                pr = (run_dir / 'pipeline_results.json').exists()
                tx = (run_dir / 'taxonomy' / 'taxonomy_predictions.csv').exists()
                nv = (run_dir / 'novelty' / 'novelty_analysis.json').exists()
                rows.append({
                    'dataset': ds,
                    'run': run_dir.name,
                    'path': str(run_dir.resolve()),
                    'modified': mtime_str,
                    'mtime': mtime,
                    'has_pipeline': pr,
                    'has_taxonomy': tx,
                    'has_novelty': nv,
                })
        except Exception:
            continue

    # Apply search filter
    if search:
        s = search.lower()
        rows = [r for r in rows if s in r['dataset'].lower() or s in r['run'].lower()]

    # Sort and cap
    rows.sort(key=lambda r: r.get('mtime', 0), reverse=True)
    rows_view = rows[: int(show_n)] if show_n and len(rows) > show_n else rows

    # Render list
    if not rows_view:
        st.info("No runs found for the current filters.")
        return

    for i, r in enumerate(rows_view):
        with st.container():
            c1, c2, c3, c4, c5 = st.columns([3, 2, 1, 1, 1])
            c1.markdown(f"**{r['dataset']} / {r['run']}**\n\n``{r['path']}``")
            c2.text(r['modified'])
            c3.markdown("results" if r['has_pipeline'] else "â€”")
            c4.markdown("taxonomy" if r['has_taxonomy'] else "â€”")
            if c5.button("Open", key=f"open_run_{i}"):
                st.session_state.prefill_results_dir = r['path']
                st.session_state.current_page_key = "results"
                st.rerun()


def show_taxonomy_viewer():
    """Display taxonomy viewer page with conflict and lineage summaries"""
    st.title("Taxonomy Viewer")
    st.info("Load taxonomy_predictions.csv (and optional tiebreak report) from a results directory.")

    col1, col2 = st.columns([2, 1])
    with col1:
        default_dir = str(Path("results").resolve())
        results_dir = st.text_input("Results directory", value=default_dir, help="Folder containing taxonomy/taxonomy_predictions.csv")
    with col2:
        refresh = st.button("Load", type="primary")

    if results_dir:
        tax_csv = Path(results_dir) / "taxonomy" / "taxonomy_predictions.csv"
        report_csv = Path(results_dir) / "taxonomy" / "taxonomy_tiebreak_report.csv"
        if not tax_csv.exists():
            st.warning(f"taxonomy_predictions.csv not found at {tax_csv}")
            return
        try:
            df = pd.read_csv(tax_csv)
        except Exception as e:
            st.error(f"Failed to read taxonomy_predictions.csv: {e}")
            return

        # Summary cards
        total = len(df)
        unique_labels = df['assigned_label'].dropna().nunique() if 'assigned_label' in df.columns else 0
        conflicts = 0
        if 'conflict_flag' in df.columns:
            try:
                conflicts = int(df['conflict_flag'].astype(str).str.lower().eq('true').sum())
            except Exception:
                conflicts = 0
        colA, colB, colC, colD = st.columns(4)
        colA.metric("Total Sequences", f"{total:,}")
        colB.metric("Unique Taxa", f"{unique_labels:,}")
        colC.metric("Conflicts", f"{conflicts:,}")
        if report_csv.exists():
            colD.download_button("Download Tiebreak Report", data=report_csv.read_bytes(), file_name="taxonomy_tiebreak_report.csv")

        st.markdown("---")

        # Filters
        with st.expander("Filters"):
            only_conflicts = st.checkbox("Show only conflicts", value=False)
            top_n = st.number_input("Top N taxa chart", min_value=5, max_value=100, value=15)

        view_df = df.copy()
        if only_conflicts and 'conflict_flag' in view_df.columns:
            try:
                view_df = view_df[view_df['conflict_flag'].astype(str).str.lower() == 'true']
            except Exception:
                pass

        # Top taxa chart
        try:
            if 'assigned_label' in view_df.columns and len(view_df) > 0:
                top_counts = view_df['assigned_label'].fillna('Unknown').value_counts().head(int(top_n))
                chart_df = pd.DataFrame({"Taxon": top_counts.index, "Count": top_counts.values})
                fig = px.bar(chart_df, x='Taxon', y='Count', title=f"Top {int(top_n)} Assigned Taxa")
                st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass

        # Simplified data table with core columns
        preferred_cols = [
            'sequence_id', 'assigned_rank', 'assigned_label', 'confidence',
            'tiebreak_winner', 'conflict_flag'
        ]
        cols = [c for c in preferred_cols if c in view_df.columns]
        st.dataframe(view_df[cols], use_container_width=True, hide_index=True)
        
        st.caption("Download the complete, unfiltered taxonomy table below.")
        
        # Download full (complete) table
        st.download_button(
            label="Download full taxonomy_predictions.csv (complete table)",
            data=tax_csv.read_bytes(),
            file_name="taxonomy_predictions.csv",
            use_container_width=True
        )


def show_about_page():
    """Display about page"""
    
    st.title("About eDNA Biodiversity Assessment System")
    
    st.markdown("""
    ## Project Overview
    
    The eDNA Biodiversity Assessment System is an end-to-end platform for identifying 
    taxonomic diversity and assessing biological richness in deep-sea environmental DNA 
    (eDNA) datasets using advanced machine learning and bioinformatics techniques.
    
    ## Key Features
    
    - **Universal Format Support**: FASTA, FASTQ, Swiss-Prot, GenBank, EMBL
    - **Large File Processing**: Supports datasets up to 2GB in size
    - **Automatic Analysis**: Intelligent sequence type detection and analysis
    - **Performance Optimized**: Parallel processing and vectorized computations
    - **Interactive Visualizations**: Real-time charts and plots
    - **Comprehensive Reports**: Detailed analysis summaries
    
    ## Technology Stack
    
    - **Python 3.13** - Core programming language
    - **BioPython** - Biological sequence analysis
    - **Streamlit** - Web interface framework
    - **Plotly** - Interactive visualizations
    - **NumPy & Pandas** - Data processing
    - **PyTorch** - Deep learning capabilities
    
    ## Support
    
    For questions, issues, or feature requests, please contact the development team.
    """)

def show_training_page():
    """Display model training page"""
    st.title("Model Training")
    st.markdown("""
    Train custom DNA embedding models using your own data. 
    Choose between **Contrastive Learning** (recommended for best performance), 
    **Autoencoders**, or standard **Transformers**.
    """)
    
    tab1, tab2 = st.tabs(["Train New Model", "Manage Models"])
    
    # --- Tab 1: Train New Model ---
    with tab1:
        st.markdown("### 1. Data Selection")
        
        # Use SRA-integrated data source selector if available
        if create_sra_data_source_selector:
            source_type, sequences_path, metadata = create_sra_data_source_selector()
            
            if metadata:
                st.info(f"Data source: {metadata.get('source', 'unknown').upper()}")
                if metadata.get('source') == 'sra':
                    st.success(f"SRA Accession: {metadata.get('accession')}")
        else:
            # Fallback to original data source selection
            data_source = st.radio("Data Source", ["Upload New File", "Select Existing Dataset"])
            
            sequences_path = None
            metadata = None
            
            if data_source == "Upload New File":
                uploaded_file = st.file_uploader("Upload FASTA File", type=['fasta', 'fa'])
                if uploaded_file:
                    # Save to temp location
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".fasta") as tmp:
                        tmp.write(uploaded_file.getvalue())
                        sequences_path = tmp.name
            else:
                # List files in datasets dir
                datasets_dir = Path(app_config.get('storage.datasets_dir', 'data/datasets'))
                if datasets_dir.exists():
                    files = list(datasets_dir.glob("*.fasta")) + list(datasets_dir.glob("*.fa"))
                    if files:
                        selected_file = st.selectbox("Select Dataset", files, format_func=lambda x: x.name)
                        sequences_path = str(selected_file)
                    else:
                        st.warning("No datasets found in storage.")
                else:
                    st.warning("Datasets directory not found.")
        
        # Labels (Optional)
        st.markdown("#### Labels (Optional)")
        st.markdown("Upload a CSV/TXT file with labels corresponding to sequences for supervised training.")
        labels_file = st.file_uploader("Upload Labels", type=['csv', 'txt'])
        labels_path = None
        if labels_file:
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(labels_file.name).suffix) as tmp:
                tmp.write(labels_file.getvalue())
                labels_path = tmp.name
        
        st.markdown("---")
        st.markdown("### 2. Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            model_type = st.selectbox(
                "Model Architecture", 
                ["Contrastive Learning", "Transformer", "Autoencoder"],
                help="Contrastive Learning is recommended for embedding generation."
            )
            
            epochs = st.number_input("Epochs", min_value=1, value=50)
            batch_size = st.number_input("Batch Size", min_value=2, value=32)
            learning_rate = st.number_input("Learning Rate", min_value=1e-6, value=1e-4, format="%.6f")
            
        with col2:
            embedding_dim = st.number_input("Embedding Dimension", min_value=32, value=256)
            
            if model_type == "Contrastive Learning":
                projection_dim = st.number_input("Projection Dimension", min_value=32, value=128)
                temperature = st.number_input("Temperature", min_value=0.01, value=0.1)
            
            device = st.selectbox("Device", ["auto", "cpu", "cuda"])
            
        model_name = st.text_input("Model Name", value=f"model_{int(time.time())}")
        
        st.markdown("---")
        
        if st.button("Start Training", type="primary"):
            if not sequences_path:
                st.error("Please select a sequence file.")
            else:
                train_model_ui(
                    sequences_path, labels_path, model_type, model_name,
                    epochs, batch_size, learning_rate, embedding_dim,
                    projection_dim if model_type == "Contrastive Learning" else None,
                    temperature if model_type == "Contrastive Learning" else None,
                    device
                )

    # --- Tab 2: Manage Models ---
    with tab2:
        show_model_management()

def train_model_ui(sequences_path, labels_path, model_type_ui, model_name,
                  epochs, batch_size, learning_rate, embedding_dim,
                  projection_dim, temperature, device):
    """Execute training from UI"""
    
    # Map UI model type to internal name
    type_map = {
        "Contrastive Learning": "contrastive",
        "Transformer": "transformer",
        "Autoencoder": "autoencoder"
    }
    model_type = type_map[model_type_ui]
    
    # Output directory
    models_dir = Path("models")
    output_dir = models_dir / model_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    status_container = st.empty()
    progress_bar = st.progress(0)
    metrics_col1, metrics_col2 = st.columns(2)
    chart_placeholder = st.empty()
    
    try:
        status_container.info("Initializing training...")
        
        # Load data
        from scripts.train_model import load_sequences, load_labels, create_model
        
        sequences = load_sequences(sequences_path)
        labels = load_labels(labels_path, sequences) if labels_path else None
        
        # Create tokenizer
        tokenizer = DNATokenizer(encoding_type='kmer', kmer_size=6)
        
        # Create model
        config_dict = {
            'embedding': {'embedding_dim': embedding_dim},
            'training': {
                'projection_dim': projection_dim,
                'temperature': temperature
            }
        }
        
        model, _ = create_model(model_type, tokenizer.vocab_size, config_dict)
        trainer = EmbeddingTrainer(model, tokenizer, device=device)
        
        # Prepare data
        train_loader, val_loader = trainer.prepare_data(
            sequences=sequences,
            labels=labels,
            validation_split=0.2,
            batch_size=batch_size
        )
        
        # Training loop
        status_container.info(f"Training {model_type_ui} model for {epochs} epochs...")
        
        history = {'train_loss': [], 'val_loss': []}
        
        for epoch in range(epochs):
            # Train one epoch
            if model_type == 'autoencoder':
                # Custom single epoch training logic would be needed here to update UI per epoch
                # For now, we'll use the trainer's method but it runs all epochs
                # To support UI updates, we'd need to modify trainer or implement loop here
                # Let's implement a simple loop here using trainer's internal methods if possible
                # Or just run the whole thing and show final result (less ideal)
                
                # Better approach: Use the trainer's methods but for 1 epoch at a time
                epoch_history = trainer.train_autoencoder(train_loader, val_loader, epochs=1, learning_rate=learning_rate)
            else:
                epoch_history = trainer.train_contrastive(train_loader, val_loader, epochs=1, learning_rate=learning_rate)
            
            # Update history
            train_loss = epoch_history['train_loss'][0]
            val_loss = epoch_history['val_loss'][0]
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            
            # Update UI
            progress = (epoch + 1) / epochs
            progress_bar.progress(progress)
            
            with metrics_col1:
                st.metric("Epoch", f"{epoch+1}/{epochs}")
            with metrics_col2:
                st.metric("Train Loss", f"{train_loss:.4f}", delta=None)
                
            # Update chart
            chart_data = pd.DataFrame({
                'Epoch': range(1, len(history['train_loss']) + 1),
                'Train Loss': history['train_loss'],
                'Val Loss': history['val_loss']
            })
            
            fig = px.line(chart_data, x='Epoch', y=['Train Loss', 'Val Loss'], title='Training Progress')
            chart_placeholder.plotly_chart(fig, use_container_width=True)
            
        # Save model
        status_container.info("Saving model...")
        trainer.save_model(str(output_dir / "model"), include_tokenizer=True)
        
        # Save metadata
        metadata = {
            'model_type': model_type,
            'epochs': epochs,
            'final_loss': history['train_loss'][-1],
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        with open(output_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f)
            
        status_container.success(f"Training complete! Model saved to {output_dir}")
        st.balloons()
        
    except Exception as e:
        status_container.error(f"Training failed: {str(e)}")
        st.exception(e)

def show_model_management():
    """Display model management interface"""
    models_dir = Path("models")
    if not models_dir.exists():
        st.info("No models found.")
        return
        
    models = [d for d in models_dir.iterdir() if d.is_dir()]
    
    if not models:
        st.info("No trained models found.")
        return
        
    st.markdown("### Trained Models")
    
    for model_dir in models:
        with st.expander(f"{model_dir.name}"):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                # Load metadata if exists
                meta_path = model_dir / "metadata.json"
                if meta_path.exists():
                    with open(meta_path, 'r') as f:
                        meta = json.load(f)
                    st.json(meta)
                else:
                    st.text("No metadata available")
                    
            with col2:
                if st.button("Delete", key=f"del_{model_dir.name}"):
                    import shutil
                    shutil.rmtree(model_dir)
                    st.rerun()

if __name__ == "__main__":
    main()